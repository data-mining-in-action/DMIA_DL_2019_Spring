{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 02: Basic Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework is heavily based on materials from [Practical DL](https://github.com/yandexdataschool/Practical_DL/) course offered by HSE, YSDA and Skoltech.\n",
    "\n",
    "The goal of this homework is simple, yet an actual implementation may take some time :). We are going to write an Artificial Neural Network (almost) from scratch. The software design of was heavily inspired by [Torch](http://torch.ch) which is the most convenient neural network environment when the work involves defining new layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework requires answering **multiple** questions in [google form](https://goo.gl/forms/4pEVo1aTtgqNd6EI2). Make sure you are entering mail address that you used for registration for DMIA DL. In this notebook you will find clear instructions on what to send and how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T14:48:03.662368Z",
     "start_time": "2019-03-17T14:48:03.352495Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from time import time, sleep\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-17T14:48:03.873626Z",
     "start_time": "2019-03-17T14:48:03.870134Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement everything in `Modules.ipynb`. Read all the comments thoughtfully to ease the pain. Please try not to change the prototypes.\n",
    "\n",
    "Do not forget, that each module should return **AND** store `output` and `gradInput`.\n",
    "\n",
    "The typical assumption is that `module.backward` is always executed after `module.forward`,\n",
    "so `output` is stored, this would be useful for `SoftMax`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tech note\n",
    "Prefer using `np.multiply`, `np.add`, `np.divide`, `np.subtract` instead of `*`,`+`,`/`,`-` for better memory handling.\n",
    "\n",
    "Example: suppose you allocated a variable \n",
    "\n",
    "```\n",
    "a = np.zeros(...)\n",
    "```\n",
    "So, instead of\n",
    "```\n",
    "a = b + c  # will be reallocated, GC needed to free\n",
    "``` \n",
    "You can use: \n",
    "```\n",
    "np.add(b,c,out = a) # puts result in `a`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T08:20:21.620166Z",
     "start_time": "2019-03-07T08:20:21.471626Z"
    }
   },
   "outputs": [],
   "source": [
    "# (re-)load layers\n",
    "%run homework_modules.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer is implemented for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T05:42:28.048985Z",
     "start_time": "2019-03-07T05:42:28.040826Z"
    }
   },
   "outputs": [],
   "source": [
    "def sgd_momentum(x, dx, config, state):\n",
    "    \"\"\"\n",
    "        This is a very ugly implementation of sgd with momentum \n",
    "        just to show an example how to store old grad in state.\n",
    "        \n",
    "        config:\n",
    "            - momentum\n",
    "            - learning_rate\n",
    "        state:\n",
    "            - old_grad\n",
    "    \"\"\"\n",
    "    \n",
    "    # x and dx have complex structure, old dx will be stored in a simpler one\n",
    "    state.setdefault('old_grad', {})\n",
    "    \n",
    "    i = 0 \n",
    "    for cur_layer_x, cur_layer_dx in zip(x, dx):\n",
    "        for cur_x, cur_dx in zip(cur_layer_x, cur_layer_dx):\n",
    "            \n",
    "            cur_old_grad = state['old_grad'].setdefault(i, np.zeros_like(\n",
    "                cur_dx\n",
    "            ))\n",
    "            \n",
    "            np.add(config['momentum'] * cur_old_grad,\n",
    "                   config['learning_rate'] * cur_dx,\n",
    "                   out=cur_old_grad)\n",
    "            cur_x -= cur_old_grad\n",
    "            i += 1     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this example to debug your code, start with logistic regression and then test other layers. You do not need to change anything here. This code is provided for you to test the layers. Also it is easy to use this code in MNIST task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T05:42:43.206722Z",
     "start_time": "2019-03-07T05:42:42.957594Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "N = 500\n",
    "\n",
    "X1 = np.random.randn(N,2) + np.array([2,2])\n",
    "X2 = np.random.randn(N,2) + np.array([-2,-2])\n",
    "\n",
    "Y = np.concatenate([np.ones(N),np.zeros(N)])[:,None]\n",
    "Y = np.hstack([Y, 1-Y])\n",
    "\n",
    "X = np.vstack([X1,X2])\n",
    "plt.scatter(X[:,0],X[:,1], c = Y[:,0], edgecolors= 'none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic regression** is already implented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T05:44:29.060039Z",
     "start_time": "2019-03-07T05:44:29.002663Z"
    }
   },
   "outputs": [],
   "source": [
    "%run homework_modules.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T05:44:48.968384Z",
     "start_time": "2019-03-07T05:44:48.963387Z"
    }
   },
   "outputs": [],
   "source": [
    "net = Sequential()\n",
    "net.add(Linear(2, 2))\n",
    "net.add(SoftMax())\n",
    "\n",
    "criterion = ClassNLLCriterion()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try **something deeper** in the cell below, for example fully-connected network consisting of Linear -> ReLU -> Linear -> LogSoftMax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T05:47:35.582751Z",
     "start_time": "2019-03-07T05:47:35.577818Z"
    }
   },
   "outputs": [],
   "source": [
    "net = Sequential()\n",
    "#######\n",
    "# Add modules here\n",
    "#######\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with batch_size = 1000 to make sure every step lowers the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T05:48:05.808275Z",
     "start_time": "2019-03-07T05:48:05.804112Z"
    }
   },
   "outputs": [],
   "source": [
    "# Iptimizer params\n",
    "optimizer_config = {'learning_rate' : 1e-1, 'momentum': 0.9}\n",
    "optimizer_state = {}\n",
    "\n",
    "# Looping params\n",
    "n_epoch = 20\n",
    "batch_size = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T05:48:06.051668Z",
     "start_time": "2019-03-07T05:48:06.045657Z"
    }
   },
   "outputs": [],
   "source": [
    "# batch generator\n",
    "def get_batches(dataset, batch_size):\n",
    "    X, Y = dataset\n",
    "    n_samples = X.shape[0]\n",
    "        \n",
    "    # Shuffle at the start of epoch\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start in range(0, n_samples, batch_size):\n",
    "        end = min(start + batch_size, n_samples)\n",
    "        batch_idx = indices[start:end]\n",
    "        yield X[batch_idx], Y[batch_idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic training loop. Examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T05:48:09.162353Z",
     "start_time": "2019-03-07T05:48:08.992900Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    for x_batch, y_batch in get_batches((X, Y), batch_size):\n",
    "        net.zeroGradParameters()\n",
    "        \n",
    "        # Forward\n",
    "        predictions = net.forward(x_batch)\n",
    "        loss = criterion.forward(predictions, y_batch)\n",
    "    \n",
    "        # Backward\n",
    "        dp = criterion.backward(predictions, y_batch)\n",
    "        net.backward(x_batch, dp)\n",
    "        \n",
    "        # Update weights\n",
    "        sgd_momentum(net.getParameters(), \n",
    "                     net.getGradParameters(), \n",
    "                     optimizer_config,\n",
    "                     optimizer_state)      \n",
    "        \n",
    "        loss_history.append(loss)\n",
    "\n",
    "    # Visualize\n",
    "    display.clear_output(wait=True)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "        \n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"#iteration\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.plot(loss_history, 'b')\n",
    "    plt.show()\n",
    "    print('Current loss: %f'%loss) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Loss stability\n",
    "\n",
    "In the cell below print the integer which answers the question: \"during which epoch training loss becomes more or less stable around minimum?\", for example: 3. Write your answer in the task 1 of the google form for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T06:08:25.239439Z",
     "start_time": "2019-03-07T06:08:25.235737Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your answer goes here. ################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using [MNIST](http://yann.lecun.com/exdb/mnist/) as our dataset. Lets start with [cool visualization](http://scs.ryerson.ca/~aharley/vis/). The most beautiful demo is the second one, if you are not familiar with convolutions you can return to it in several lectures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T05:48:56.293545Z",
     "start_time": "2019-03-07T05:48:56.290117Z"
    }
   },
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T05:48:57.733167Z",
     "start_time": "2019-03-07T05:48:56.526070Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Fetch MNIST dataset and create a local copy.\n",
    "if os.path.exists('mnist.npz'):\n",
    "    with np.load('mnist.npz', 'r') as data:\n",
    "        X = data['X']\n",
    "        y = data['y']\n",
    "else:\n",
    "    mnist = mnist = fetch_openml('mnist_784')\n",
    "    X, y = mnist.data / 255.0, mnist.target\n",
    "    np.savez('mnist.npz', X=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encode the labels first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T05:49:00.480587Z",
     "start_time": "2019-03-07T05:49:00.445216Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your code goes here. ################################################\n",
    "raise NotImplementedError\n",
    "Y = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Compare** `ReLU`, `ELU`, `LeakyReLU`, `SoftPlus` activation functions. \n",
    "You would better pick the best optimizer params for each of them, but it is overkill for now. Use an architecture of your choice for the comparison.\n",
    "- Hint: logloss for MNIST should be around 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T22:25:58.408493Z",
     "start_time": "2019-03-06T22:25:58.403590Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer_config = {'learning_rate' : 1e-1, 'momentum': 0.9}\n",
    "optimizer_state = {}\n",
    "\n",
    "# Looping params\n",
    "n_epoch = 20\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T22:27:00.897558Z",
     "start_time": "2019-03-06T22:25:58.413157Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = ClassNLLCriterion()\n",
    "\n",
    "net = Sequential()\n",
    "net.add(Linear(784, 50))\n",
    "net.add(ReLU())\n",
    "net.add(Linear(50, 10))\n",
    "net.add(SoftMax())\n",
    "\n",
    "loss_relu = []\n",
    "\n",
    "t = time()\n",
    "for i in range(n_epoch):\n",
    "    for x_batch, y_batch in get_batches((X,Y) , batch_size):\n",
    "        net.zeroGradParameters()\n",
    "        predictions = net.forward(x_batch)\n",
    "        loss = criterion.forward(predictions, y_batch)\n",
    "        dp = criterion.backward(predictions, y_batch)\n",
    "        net.backward(x_batch, dp)\n",
    "        sgd_momentum(net.getParameters(), \n",
    "                     net.getGradParameters(), \n",
    "                     optimizer_config,\n",
    "                     optimizer_state)\n",
    "    loss_relu.append(loss)\n",
    "print('Time: {}'.format(time() - t))\n",
    "print('ReLU logloss: {}'.format(loss_relu[-5:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T22:28:05.137045Z",
     "start_time": "2019-03-06T22:27:00.900391Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = ClassNLLCriterion()\n",
    "\n",
    "net = Sequential()\n",
    "net.add(Linear(784, 50))\n",
    "net.add(ELU())\n",
    "net.add(Linear(50, 10))\n",
    "net.add(SoftMax())\n",
    "\n",
    "loss_elu = []\n",
    "t = time()\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    for x_batch, y_batch in get_batches( (X,Y) , batch_size):\n",
    "        net.zeroGradParameters()        \n",
    "        predictions = net.forward(x_batch)        \n",
    "        loss = criterion.forward(predictions, y_batch)        \n",
    "        dp = criterion.backward(predictions, y_batch)\n",
    "        net.backward(x_batch, dp)\n",
    "        sgd_momentum(net.getParameters(), \n",
    "                     net.getGradParameters(), \n",
    "                     optimizer_config,\n",
    "                     optimizer_state)      \n",
    "    loss_elu.append(loss)\n",
    "        \n",
    "print('Time: {}'.format(time() - t))\n",
    "print('ELU logloss: {}'.format(loss_elu[-5:])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T22:29:09.928878Z",
     "start_time": "2019-03-06T22:28:05.139838Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = ClassNLLCriterion()\n",
    "\n",
    "net = Sequential()\n",
    "net.add(Linear(784, 50))\n",
    "net.add(LeakyReLU(0.02))\n",
    "net.add(Linear(50, 10))\n",
    "net.add(SoftMax())\n",
    "\n",
    "loss_leakyRelu = []\n",
    "t = time()\n",
    "\n",
    "for i in range(n_epoch):\n",
    "    for x_batch, y_batch in get_batches( (X,Y) , batch_size):\n",
    "        net.zeroGradParameters()        \n",
    "        predictions = net.forward(x_batch)        \n",
    "        loss = criterion.forward(predictions, y_batch)        \n",
    "        dp = criterion.backward(predictions, y_batch)\n",
    "        net.backward(x_batch, dp)\n",
    "        sgd_momentum(net.getParameters(), \n",
    "                     net.getGradParameters(), \n",
    "                     optimizer_config,\n",
    "                     optimizer_state)      \n",
    "    loss_leakyRelu.append(loss)\n",
    "        \n",
    "print('Time: {}'.format(time() - t))\n",
    "print('ELU logloss: {}'.format(loss_leakyRelu[-5:])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T22:29:10.216340Z",
     "start_time": "2019-03-06T22:29:09.940014Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_relu)\n",
    "plt.plot(loss_elu)\n",
    "plt.plot(loss_leakyRelu)\n",
    "plt.legend(('ReLU', 'ELU', 'LeakyReLU'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: ReLU, ELU, LeakyReLu\n",
    "\n",
    "Print here all the activation functions you checked in the order of increasing performance, for example: elu, leaky_relu, relu (mind the format of the answer). Write your answer in the **task 2** of the google form for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer goes here. ################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally**, use all your knowledge to build a super cool model on this dataset, do not forget to split dataset into train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T05:51:25.408196Z",
     "start_time": "2019-03-07T05:51:23.592780Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T05:51:25.453461Z",
     "start_time": "2019-03-07T05:51:25.449754Z"
    }
   },
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your network in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T22:29:12.103641Z",
     "start_time": "2019-03-06T22:29:12.093409Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = ClassNLLCriterion()\n",
    "\n",
    "net = Sequential()\n",
    "####### \n",
    "# Add modules here\n",
    "#######\n",
    "\n",
    "optimizer_config = {'learning_rate' : 1e-1, 'momentum': 0.9}\n",
    "optimizer_state = {}\n",
    "\n",
    "n_epoch = 20\n",
    "batch_size = 128\n",
    "\n",
    "loss_history_train, loss_history_val = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T22:29:51.330745Z",
     "start_time": "2019-03-06T22:29:12.107614Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(n_epoch):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_batches = 0\n",
    "    for x_batch, y_batch in get_batches((X_train,y_train) , batch_size):\n",
    "        \n",
    "        net.zeroGradParameters()  \n",
    "        predictions = net.forward(x_batch)\n",
    "        train_loss += criterion.forward(predictions, y_batch)\n",
    "        y_pred = [np.argmax(pred) for pred in predictions]\n",
    "        y_batch_test = [np.argmax(batch_i) for batch_i in y_batch]\n",
    "        train_acc += accuracy_score(y_pred, y_batch_test)\n",
    "        train_batches += 1\n",
    "        dp = criterion.backward(predictions, y_batch)\n",
    "        net.backward(x_batch, dp)\n",
    "        \n",
    "        sgd_momentum(net.getParameters(), \n",
    "                     net.getGradParameters(), \n",
    "                     optimizer_config,\n",
    "                     optimizer_state)      \n",
    "        \n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "    val_batches = 0\n",
    "    for x_batch, y_batch in get_batches( (X_test, y_test) , batch_size):\n",
    "        predictions = net.forward(x_batch)\n",
    "        val_loss += criterion.forward(predictions, y_batch)\n",
    "        y_pred = [np.argmax(pred) for pred in predictions]\n",
    "        y_batch_test = [np.argmax(batch_i) for batch_i in y_batch]\n",
    "        val_acc += accuracy_score(y_pred, y_batch_test)\n",
    "        val_batches += 1\n",
    "    \n",
    "    print(\"training loss: {:.6f}, train accuracy: {:.5f}, validation accuracy: {:.5f}, epoch: {}\".format(train_loss / train_batches,\\\n",
    "        train_acc / train_batches, val_acc / val_batches, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Validation accuracy & Task 4: Validation loss\n",
    "\n",
    "Print here your **validation accuracy** (ex: 0.75) and **validation loss** (ex: 0.1), both rounded up up to 2-nd digit after decimal point. Write your answers in the **tasks 3 (validation accuracy) and 4 (validation loss)** of the google form for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-07T07:58:26.165430Z",
     "start_time": "2019-03-07T07:58:26.162267Z"
    }
   },
   "outputs": [],
   "source": [
    "# Your answer goes here. ################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
